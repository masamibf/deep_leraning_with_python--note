回归（regression）：     连续值预测（如：房子、货物的价格）

分类（classification）： 离散值预测（如：有无疾病）

监督学习（supervised learning）：样本已被告知正确答案，有标签

无监督学习（unsupervised learning）：样本未被告知正确答案，无标签

聚类算法（clustering algorithm）：如谷歌新闻，搜索大量新闻，按照内容分类

=============================================================

Keras训练步骤：
    数据格式化为张量
    数据标准化
    建立深度模型
    选用策略，即目标函数
    采用优化器，编译和训练模型

建立模型有两种方式：
    向layer添加list方式
    通过 .add() 方式一层层添加，一个 .add() 为一层

Keras中最简单的模型是 Sequential（顺序模型）：多个网络层的线性堆叠，无分叉结构
    network.add()  　　　   　添加层
    network.compile()　　　　 模型训练的BP模式设置
    network.fit()　　　　　　  模型训练参数设置+训练
    network.evaluate()   　　 模型评估
    network.predict()　　　　 模型预测

卷积神经网络的层级结构
   数据输入层/ Input layer
　　卷积计算层/ CONV layer
　　ReLU激励层/ ReLU layer
　　池化层/     Pooling layer
　　全连接层/   FC layer

=============================================================

	数据输入层要做的处理主要是对原始图像数据进行预处理，其中包括：
　　	去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。

　　    归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰

　　    PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化

	卷积计算层，有两个关键操作：
　　	 局部关联。每个神经元看做一个滤波器(filter)

　　	 窗口(receptive field)滑动， filter对局部数据计算

	ReLU函数：ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。它的特点是收敛快，求梯度简单，但较脆弱

	池化层夹在连续的卷积层中间，用于压缩数据和参数的量，减小过拟合。
	简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像

	全连接层：两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的

	卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，
	只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。CNN一个非常重要的特点就是头重脚轻（越往输入权值越小，
	越往输出权值越多），呈现出一个倒三角的形态，这就很好地避免了BP神经网络中反向传播的时候梯度损失得太快。
    卷积神经网络CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，
    所以在使用CNN时，避免了显式的特征抽取，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，
    所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。
    卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，
    权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。

=============================================================

epoch:当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch

CIFAR10 数据集有 50000 张训练图片，10000 张测试图片。现在选择 Batch Size = 256 对模型进行训练。

	每个 Epoch 要训练的图片数量： 50000
	训练集具有的 Batch 个数： 50000 / 256 = 195 + 1 = 196
	每个 Epoch 需要完成的 Batch 个数： 196
	每个 Epoch 具有的 Iteration 个数： 196
	每个 Epoch 中发生模型权重更新的次数： 196
	训练 10 代后，模型权重更新的次数： 196 * 10 = 1960
	不同代的训练，其实用的是同一个训练集的数据。第 1 代和第 10 代虽然用的都是训练集的五万张图片，
	但是对模型的权重更新值却是完全不同的。因为不同代的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。

=============================================================

训练集（train set） ―― 用于模型拟合的数据样本。
    学生的课本，学生根据课本里的内容来掌握知识。

验证集（development set）―― 是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。
    作业，通过作业可以知道 不同学生学习情况、进步的速度快慢。

测试集 ―― 用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。
    考试，考的题是平常都没有见过，考察学生举一反三的能力。

传统上，一般三者切分的比例是6：2：2，验证集并不是必须的。

=============================================================

One-Hot编码，又称为一位有效编码，主要是采用N位状态寄存器来对N个状态进行编码，
每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。

One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。
然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。

==============================================================

如果要对N个类别的数据点进行分类，网络的最后一层应该是大小为N的Dense层

对于单标签、多分类问题，网络的最后一层应该使用softmax激活，这样可以输出在N个输出类别上的概率分布

如果需要将数据划分到许多类别中，应该避免使用太小的中间层，以免在网络中造成 信息瓶颈

=============================================================

回归问题使用的损失函数与分类问题不同。回归常用的损失函数是均方误差（MSE）

常见的回归指标是平均绝对误差（MAE）

如果输入数据的特征具有不同的取值范围，应该先进行预处理，对每个特征单独进行缩放

如果可用的数据很少，使用K折验证可以可靠地评估模型，隐藏层较少避免过拟合

=============================================================

防止过拟合的常用方法：
    获取更多的训练数据
    减小网络容量
    添加权重正则化
    添加dropout

=============================================================

对于平衡分类问题（每个类别的可能性相同），精度 和 接受者操作特征曲线 是常用指标

对于类别不平衡的问题，可以使用 准确率 和 召回率

=============================================================






